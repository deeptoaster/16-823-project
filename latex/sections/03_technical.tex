\section{Method}\label{sec:technical}
\subsection{Photometric Stereo With Multiple Lights}
In the original formulation of photometric stereo~\cite{woodham1979}, a minimum
of $N \ge 3$ frames of an diffuse object are captured. The lighting of a single
frame $1 \le i \le N$ is assumed to come from a single directional source
$\light_i$, where $\hat{\light}_i = \frac{\light_i}{\|\light_i\|}$ is the
vector pointing in the direction of the source (opposite the incoming rays) and
$\|\light_i\|$ is its intensity. Then, for each pixel of value $\pixel_i$ that
images a point on the object with surface normal $\hat{\point}$ and albedo
$\albedo$,
\begin{equation}
  \pixel_i = \light_i^\intercal \rho \hat{\point}
\end{equation}
If, instead of a single light, we have $L$ sources possibly incident on the
point in a single frame, this becomes
\begin{equation} \label{eq:photometric-stereo}
  \pixel_i = \rho \sum_{j = 1}^L \occlusion_{j, i} \light_{j, i}^\intercal \hat{\point}
           = \left(\sum_{j = 1}^L \occlusion_{j, i} \light_{j, i}^\intercal\right) \rho \hat{\point}
\end{equation}
as the sum of the contribution of each source $1 \le j \le L$. Here,
$\occlusion_{j, i}$ is an indicator of whether light from source $j$ contacts
the point or is occluded. As in the original formulation, the problem is
underconstrained unless there are at least $N \ge 3$ \emph{captures} of each
pixel, with $\light_i$ not all coplanar and each nonzero (i.e., not with all
lights involved occluded). 

Let $\tilde{\light}_i = \sum_{j = 1}^L \occlusion_{j, i} \light_{j, i}$ and
$\point = \rho \hat{\point}$ for brevity. Then
Equation~\ref{eq:photometric-stereo} can be written as a system of linear
equations that can be solved with least-squares methods, allowing us to recover
albedo and surface normal for each pixel:
\begin{align}
  \begin{bmatrix}
    \pixel_1 \\
    \vdots \\
    \pixel_N
  \end{bmatrix}             &= \begin{bmatrix}
    \tilde{\light}_1^\intercal \\
    \vdots \\
    \tilde{\light}_N^\intercal
  \end{bmatrix} \point \\
  \pixels                   &= \lights \point \\
  \lights^\intercal \pixels &= \lights^\intercal \lights \point \\
  \point                    &= {\left(\lights^\intercal \lights\right)}^{-1} \lights^\intercal \pixels \label{eq:pseudoinverse}
\end{align}
Continuity assumptions can then be used to recover a depth map of the scene
from the computed surface normals.
\subsection{Mirror Relationships}
A scene with one directional light and one plane mirror can be analyzed as one
with two lights that are related to each other by reflection across the mirror
plane.

Ignoring optical properties such as polarization, light arriving on the object
surface after reflecting off a perfect mirror from the direct source is
indistinguishable from light arriving from the image of the source across the
mirror plane. That is, the vector $\light_i'$ pointing from the surface to the
image of the light source can be expressed as
\begin{equation} \label{eq:light-reflection}
  \light_i' = \light_i - 2 \light_i^\parallel
            = \light_i - 2 \left(\light_i^\intercal \mirror\right) \mirror
\end{equation}
where $\mirror$ is the unit normal of the fixed scene mirror. This means that
the light from the source and mirror in a single frame can be
combined as
\begin{equation} \label{eq:combinations}
  \tilde{\light}_i = \occlusion_i \light_i + \occlusion_i' \light_i'
                   = \begin{cases}
    \light_i                                                       & \textrm{mirror occluded} \\
    \light_i - 2 \left(\light_i^\intercal \mirror\right) \mirror   & \textrm{direct occluded} \\
    2 \light_i - 2 \left(\light_i^\intercal \mirror\right) \mirror & \textrm{neither occluded} \\
    0                                                              & \textrm{both occluded}
  \end{cases}
\end{equation}
The $\tilde{\light}_i = 0$ case is the easiest to detect, as it results in a
zero-valued pixel under ideal conditions. To address this situation, we can
either take a sufficiently large number $N$ of captures and ignore the ones
that result in a zero value for any given pixel, or use shape or shading
regularization techniques with a sparse linear solver as in~\cite{hernandez}.
We ignore such cases for now, with the hope that we are able to place the light
source or mirror in such a way that either the direct light source or its
reflection reaches every pixel of interest on the image.
\begin{figure}
  \includesvg[width=\columnwidth]{images/light-reflection.svg}
  \caption{The light incident on the object from a directional source reflected
  across a mirror is analyzed as light from the image of the source. Note that
  arrows point in the direction of the vectors used in the algorithm, which is
  opposite the direction of light travel. Vector lengths are not to
  scale.}\label{fig:light-reflection}
\end{figure}
\subsection{Computing Normals and Depth}
For the other three cases, we compute $\point$ $3^N$ times, one for each case
for each of three captures. At this point, the tasks is to determine, for each
pixel location, which of the $3^N$ combinations of lights induces the correct
normal---that is, the normal that correctly explains the values observed for
that pixel in each of the frames. The various approaches applied to this task
are described in Section~\ref{sec:implementation}. Once the correct candidate
$\point$ is chosen, it is straightforward to recover $z$ according to the usual
depth-from-normal algorithm.

Another consideration in our system is that some of the pixels captured may
image a mirror reflection of the object, rather than a point on the object
directly. We can account for this by pre-segmenting the part of the capture
that images the mirror and constructing a virtual camera position---the image
of the actual camera position reflected across the mirror plane---and using it
to separately perform photometric stereo on the reflected scene. Note that
unlike with the directional light source, we cannot assume the camera, whether
orthographic or projective, to be infinitely distant from the scene. Therefore,
a slightly different virtual camera is required for every pixel captured.

